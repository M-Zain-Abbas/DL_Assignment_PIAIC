{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flowers Recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python378jvsc74a57bd06dbd9c5e6c757ece76867b80fa0e7fe6cb6ac0ab649084a18549b9b068069f92",
      "display_name": "Python 3.7.8 64-bit"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXgJ6uT1NydQ"
      },
      "source": [
        "Assignment: Flowers Recognition <br>\r\n",
        "Dataset Description:<br>\r\n",
        "\r\n",
        "This dataset contains 4242 images of flowers.<br>\r\n",
        "The data collection is based on the data flicr, google images, yandex images.<br>\r\n",
        "You can use this datastet to recognize plants from the photo.<br>\r\n",
        "\r\n",
        "Attribute Information:<br>\r\n",
        "The pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion.<br>\r\n",
        "For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. <br>\r\n",
        "<b>Also explore how to resize images in tensorflow and then resize all the images to a same size. </b> <br>\r\n",
        "This is a Multiclass Classification Problem.<br>\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7vy-ktuOKJH"
      },
      "source": [
        "WORKFLOW : <br>\r\n",
        "Load Data <br>\r\n",
        "Split into 60 and 40 ratio.<br>\r\n",
        "Encode labels.<br>\r\n",
        "Create Model<br>\r\n",
        "Compilation Step (Note : Its a Multiclass Classification problem , select loss , metrics according to it)<br>\r\n",
        "Train the Model.<br>\r\n",
        "If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .<br>\r\n",
        "Prediction should be > 85%<br>\r\n",
        "Evaluation Step<br>\r\n",
        "Prediction<br>\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri3Bg5qfPRic"
      },
      "source": [
        "Data : <br>\r\n",
        "https://drive.google.com/file/d/1-OX6wn5gA-bJpjPNfSyaYQLz-A-AB_uj/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTtg3WuGTA1o"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "daisy\n",
            "dandelion\n",
            "rose\n",
            "sunflower\n",
            "tulip\n"
          ]
        }
      ],
      "source": [
        "\n",
        "datafiles = Path(\"D:/Coding for AI/Python/PIAIC/Assignments/AI-Q2-learning-resources-master/AI-Q2-learning-resources-master/DLAssignments/flowers\")\n",
        "\n",
        "\n",
        "flowers = []\n",
        "features = []\n",
        "labels = []\n",
        "for file in datafiles.iterdir():\n",
        "    flowers.append(file.name)\n",
        "    print(file.name)\n",
        "    for img_path in file.iterdir():\n",
        "        if img_path.name.endswith(\"jpg\"):\n",
        "            labels.append(file.name)\n",
        "            img_arr = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
        "            img_arr = cv2.resize(img_arr, (150,150))\n",
        "            features.append(img_arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "np.savez_compressed(\"flower_data\", features, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation = \"softmax\"))\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "x=features.reshape(len(features), 150,150,3)/255.0\n",
        "one_hot=LabelBinarizer()\n",
        "y=one_hot.fit_transform(labels)\n",
        "x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.4, random_state=42, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "21/21 [==============================] - 138s 6s/step - loss: 1.7257 - accuracy: 0.2160\n",
            "Epoch 2/50\n",
            "21/21 [==============================] - 127s 6s/step - loss: 1.3647 - accuracy: 0.3790\n",
            "Epoch 3/50\n",
            "21/21 [==============================] - 144s 7s/step - loss: 1.0572 - accuracy: 0.5593\n",
            "Epoch 4/50\n",
            "21/21 [==============================] - 91s 4s/step - loss: 0.9554 - accuracy: 0.6149\n",
            "Epoch 5/50\n",
            "21/21 [==============================] - 111s 5s/step - loss: 0.9459 - accuracy: 0.6159\n",
            "Epoch 6/50\n",
            "21/21 [==============================] - 94s 4s/step - loss: 0.8047 - accuracy: 0.7039\n",
            "Epoch 7/50\n",
            "21/21 [==============================] - 84s 4s/step - loss: 0.8003 - accuracy: 0.6867\n",
            "Epoch 8/50\n",
            "21/21 [==============================] - 86s 4s/step - loss: 0.7885 - accuracy: 0.6920\n",
            "Epoch 9/50\n",
            "21/21 [==============================] - 89s 4s/step - loss: 0.7087 - accuracy: 0.7255\n",
            "Epoch 10/50\n",
            "21/21 [==============================] - 86s 4s/step - loss: 0.5837 - accuracy: 0.7775\n",
            "Epoch 11/50\n",
            "21/21 [==============================] - 87s 4s/step - loss: 0.5044 - accuracy: 0.8099\n",
            "Epoch 12/50\n",
            "21/21 [==============================] - 84s 4s/step - loss: 0.4832 - accuracy: 0.8255\n",
            "Epoch 13/50\n",
            "21/21 [==============================] - 83s 4s/step - loss: 0.4131 - accuracy: 0.8522\n",
            "Epoch 14/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.4000 - accuracy: 0.8546\n",
            "Epoch 15/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.3078 - accuracy: 0.8902\n",
            "Epoch 16/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.1762 - accuracy: 0.9423\n",
            "Epoch 17/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.1323 - accuracy: 0.9571\n",
            "Epoch 18/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.1091 - accuracy: 0.9625\n",
            "Epoch 19/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0904 - accuracy: 0.9692\n",
            "Epoch 20/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.1303 - accuracy: 0.9611\n",
            "Epoch 21/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0573 - accuracy: 0.9851\n",
            "Epoch 22/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0558 - accuracy: 0.9856\n",
            "Epoch 23/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0550 - accuracy: 0.9816\n",
            "Epoch 24/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.0317 - accuracy: 0.9932\n",
            "Epoch 25/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.0208 - accuracy: 0.9947\n",
            "Epoch 26/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0143 - accuracy: 0.9975\n",
            "Epoch 27/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.0097 - accuracy: 0.9972\n",
            "Epoch 28/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.0068 - accuracy: 0.9993\n",
            "Epoch 29/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0076 - accuracy: 0.9989\n",
            "Epoch 30/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0209 - accuracy: 0.9949\n",
            "Epoch 31/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0125 - accuracy: 0.9955\n",
            "Epoch 32/50\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.0073 - accuracy: 0.9994\n",
            "Epoch 33/50\n",
            "21/21 [==============================] - 83s 4s/step - loss: 0.0044 - accuracy: 0.9992\n",
            "Epoch 34/50\n",
            "21/21 [==============================] - 78s 4s/step - loss: 0.0253 - accuracy: 0.9963\n",
            "Epoch 35/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.0123 - accuracy: 0.9985\n",
            "Epoch 36/50\n",
            "21/21 [==============================] - 88s 4s/step - loss: 0.0162 - accuracy: 0.9988\n",
            "Epoch 37/50\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.0040 - accuracy: 0.9994\n",
            "Epoch 38/50\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.0020 - accuracy: 0.9994\n",
            "Epoch 39/50\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.0027 - accuracy: 0.9979\n",
            "Epoch 40/50\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.0056 - accuracy: 0.9981\n",
            "Epoch 41/50\n",
            "21/21 [==============================] - 82s 4s/step - loss: 0.0025 - accuracy: 0.9995\n",
            "Epoch 42/50\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.0030 - accuracy: 0.9987\n",
            "Epoch 43/50\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.0044 - accuracy: 0.9983\n",
            "Epoch 44/50\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.0034 - accuracy: 0.9985\n",
            "Epoch 45/50\n",
            "21/21 [==============================] - 75s 4s/step - loss: 0.0032 - accuracy: 0.9991\n",
            "Epoch 46/50\n",
            "21/21 [==============================] - 76s 4s/step - loss: 0.0029 - accuracy: 0.9993\n",
            "Epoch 47/50\n",
            "21/21 [==============================] - 75s 4s/step - loss: 0.0026 - accuracy: 0.9992\n",
            "Epoch 48/50\n",
            "21/21 [==============================] - 97s 5s/step - loss: 0.0028 - accuracy: 0.9976\n",
            "Epoch 49/50\n",
            "21/21 [==============================] - 155s 7s/step - loss: 0.0031 - accuracy: 0.9993\n",
            "Epoch 50/50\n",
            "21/21 [==============================] - 158s 8s/step - loss: 9.5823e-04 - accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "training_model = model.fit(x_train, y_train, epochs=50, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55/55 [==============================] - 17s 310ms/step - loss: 2.6335 - accuracy: 0.6699\n"
          ]
        }
      ],
      "source": [
        "m, acc = model.evaluate(x_test,y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 66.99422001838684\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy =', acc*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}